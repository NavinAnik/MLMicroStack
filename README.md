# MicroML: Microservices-Based Machine Learning Platform

MicroML is a modular, scalable, and production-ready machine learning system designed using microservices and PostgreSQL. It separates each phase of the ML lifecycle into containerized services, each owning its own database and logic. This setup allows for independent development, deployment, and scaling of each component.

---

## ğŸ”§ Architecture Overview

```
        [User Events, Products, Logs]
                    |
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  data-ingestion-service â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ data-preprocessing-service       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  feature-store-service  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ model-training-service  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  model-serving-service  â”‚  â†â”€â”€â”€ Client/API Gateway
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ model-monitoring-serviceâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
             [ alert-service ]
```

Each service has its own **dedicated PostgreSQL database** to ensure isolation, scalability, and independent deployment.

---

## ğŸ“¦ Repo Structure

```
ml-microservice-project/
â”œâ”€â”€ docker-compose.yml                   # Compose file to run services + DBs
â”œâ”€â”€ README.md                            # Project overview and documentation
â”œâ”€â”€ .env                                 # Global env variables (non-sensitive)
â”‚
â”œâ”€â”€ inference_service/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                      # FastAPI app entry point
â”‚   â”‚   â”œâ”€â”€ model_loader.py             # Loads model.pkl
â”‚   â”‚   â”œâ”€â”€ schemas.py                  # Pydantic request/response models
â”‚   â”‚   â””â”€â”€ db.py                       # SQLAlchemy DB connection
â”‚   â”œâ”€â”€ model/                          # Folder for saved model
â”‚   â”‚   â””â”€â”€ model.pkl
â”‚   â”œâ”€â”€ migrations/                     # Alembic migrations
â”‚   â”œâ”€â”€ Dockerfile                      # Build container for inference
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env                            # Local DB creds for this service
â”‚
â”œâ”€â”€ training_pipeline/
â”‚   â”œâ”€â”€ train.py                        # Training script
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ db.py                           # SQLAlchemy training DB connection
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”œâ”€â”€ migrations/                     # DB schema for training metadata
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ feature_store/
â”‚   â”œâ”€â”€ service.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ monitoring_service/
â”‚   â”œâ”€â”€ monitor.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ logging.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ data_ingestion_service/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ preprocessing_service/
â”‚   â”œâ”€â”€ clean.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ airflow_dag.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ db/
    â”œâ”€â”€ init_ingestion.sql
    â”œâ”€â”€ init_training.sql
    â”œâ”€â”€ init_inference.sql
    â”œâ”€â”€ ...                             # One init.sql per service DB
```

---

## ğŸ› ï¸ Tech Stack

| Layer               | Tool/Framework            |
|---------------------|---------------------------|
| API & Inference     | FastAPI, Uvicorn          |
| ML Training         |      |
| Containerization    | Docker, Docker Compose    |
| DB per Service      | PostgreSQL (one per svc)  |
| Orchestration       | Airflow, Prefect (optional)|
| Monitoring          | Prometheus, ELK, Custom   |
| Migration Tooling   | Alembic (per service)     |

---

## ğŸš€ Getting Started

1. Clone the repository:
```bash
git clone https://github.com/your-org/ml-microservice-project.git
cd ml-microservice-project
```

2. Start the entire stack:
```bash
docker-compose up --build
```

3. Access services:
- Inference: http://localhost:8000/docs
- PostgreSQL DBs: Accessible via mapped ports (5433â€“5438)

---

## ğŸ“„ License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.